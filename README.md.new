# Notion FAQ Chatbot

A prototype React-based chatbot UI that demonstrates how to create a documentation-powered FAQ assistant using vector embeddings and LLMs.

## Configuration

Before using the chatbot with the real backend, you need to:

1. Configure your Gemini API key in `src/config.js`
2. Ensure your backend API is running at the configured endpoint

## How This Chatbot Works

This project demonstrates a chatbot that can answer questions based on Notion documentation:

1. **Vector Embedding Process** (to be implemented in backend):
   - Notion documentation is exported as markdown
   - Text is split into chunks and converted to vector embeddings
   - Vectors are stored in MongoDB along with their source information

2. **Question Answering Flow**:
   - User submits a question via the chatbot UI
   - Question is converted to a vector embedding
   - Vector similarity search finds relevant documentation chunks
   - These chunks are sent to Gemini API as context along with the question
   - Gemini generates a response based on the provided context
   - Response is displayed to the user along with source references

## API Integration

The chatbot connects to your backend API with the following endpoint and payload:

```
POST http://localhost/api/gemini/chat-gemini
```

Request body:
```json
{
  "query": "user's question here",
  "limit": 10,
  "threshold": 0.8
}
```

Headers:
```
Content-Type: application/json
Accept: application/json
X-API-KEY: YOUR_GEMINI_API_KEY
```
